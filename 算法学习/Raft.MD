# 前言
为什么需要共识算法，是为了解决什么问题？

假定现在有一家企业，公司刚刚成立，用户数据量不大，QPS也较小，公司为了节省成本，将所有的服务都放在了一台性能较好的服务器上。随着公司发展，逐渐步入正轨，公司的用户量也随之增长，这个时候，公司的技术人才，通过**拆分服务**，将不同的服务放到了不同的服务器上，使得**服务器的职能单一**职责化，短时间解决了用户增长的问题，但同时也带了另一个问题，单机服务的性能瓶颈受限于服务器的硬件限制，初期我们可以通过更换更好的硬件来解决问题(**垂直扩容**)，但硬件的更换的**边际成本**随着硬件质量上升**越来越高**，使得垂直扩容不再是最优选择(硬件的技术迭代可能也跟不上用户增长)。这个时候，公司的技术大佬再次出手，将原本的服务再次拆分，使得服务职能也单一化(微服务诞生)，再一次提升能够承载的用户量；但是随着用户增长，很快，单机的微服务也不能满足需求了，此时，大佬就想:“三个臭皮匠，顶个诸葛亮”，既然一台机器不行，我就用多台(**水平扩容**)，大佬便把单机的服务，变成了**分布式的服务**，使得多台服务器共同去承担一项任务。但是分布式的服务也要面对一些问题，怎么去让大家一起工作而不打架？

上述这家公司的发展历程中，遇到的问题可以总结为以下几点：
> 1. 如何应对日益增长的用户需求和数据？
     > 	1. 垂直扩容：更换更好的机器---机器的性能存在上限，边际成本高。
> 	2. 水平扩容：增加更多的机器---机器间如何协调工作，保持数据一致。
> 2. 如何在复杂的网络环境中协调一致多机器工作？
     > 	1. 一主多从：一写多读，主的写压力大。
> 	2. 多主多从：多写多读，存在写冲突。
> 3. 多台机器间何时保证状态一致？
     > 	1. 状态立即一致
> 	2. 状态最终一致
> 4. 网络中的节点可能存在哪些状态？
     > 	1. 可读可写
> 	2. 只读
> 	3. 不可用

为了解决第三个问题，便诞生了**使得多台机器的状态最终一致，并允许局部失败的算法---一致性算法(共识算法)**

## 共识算法
### Paxos
> Paxos算法是莱斯利·兰伯特于1990年提出的一种基于消息传递的一致性算法。这个算法被认为是类似算法中最有效的。

Paxos算法虽然是一致性算法中，目前为止最有效的算法，但也正是因为如此，导致 Paxos算法较难理解，在工程实践上，也因其复杂程度导致很难展开。

### Raft
Raft是一种更为简单方便易于理解的分布式算法，主要解决了分布式中的一致性问题。相比传统的Paxos算法，Raft将大量的计算问题分解成为了一些简单的相对独立的子问题。

## 复制状态机
![[复制状态机.png]](images/复制状态机.png)
一致性算法的目标是保证集群上所有节点的状态一致，而节点要执行的指令通常可以分为两种**读和写**，但只有写指令会去改变节点状态，所以，为了保证节点状态一致，我们通常情况下只需保证写一致就可以了，而写一致，则只需要保证每个节点执行的写指令一致即可。

在理想状态下，我们肯定希望我们的写指令一经客户端发送，所有的服务节点就立即执行，达到**状态立即一致**的目标，就像单机一样。但是，因为网络状况的复杂性，比如：网络时延，拥塞，丢包等，往往导致我们不同节点拿到的指令的顺序/时间可能不同，以致于达不到**状态立即一致**的目标，甚至可能因为数据丢失连**所有节点状态最终一致**的目标都可能达不到，所以我们退而求其次，希望能够通过一种方式，保证我们**大多数节点**能够按照我们发送的指令顺序去执行写指令，达到**大多数节点状态最终一致性**的目标。

这种方式就是日志复制，我们不要求所有的节点的写指令立即被执行，而是节点在收到指令时，首要保证的是指令的顺序一致,然后再去执行指令。那要如何保证指令顺序一致，这就需要一个节点来专门组织写指令，然后再将组织好的指令复制给其他节点，其他节点也按照这个指令顺序执行即可。因此，整个集群就看起来就达到了最终一致性。

### 特点
+ 安全性保证：在非拜占庭错误情况下，包括网络延迟/分区/丢包/冗余/乱序等错误都可以保证正确。
+ 基本可用性：集群中只要有大多数的机器可运行并且能够相互通信，和客户端通信，就可以保证可用。
+ 不依赖时序来保证一致性：不依赖物理时钟或者极端的消息延迟来保证数据的一致性。
+ 快速响应：客户端的请求响应速度不能依赖于集群中最慢的节点。


# Raft 算法详解
通过以上部分，我们可以定义一个一致性算法需要解决的问题：
> 在保证**安全性/基本可用性/不依赖时序**的前提下，使得客户端的**写指令**，能够**快速响应**并达到集群大多数节点**状态最终一致性**的目标。

那要如何解决这个问题呢？
***
1. 我们需要一个节点来保证写指令的一致性，因此所有来自客户端的写指令，都会重定向到这个节点，由这个节点组织好后发送给其他节点；我们称这个节点为领导者(Leader)，其他的节点为跟随者(Follower)。
2. 领导者要先自己写入日志，然后在发送给跟随者，当半数以上(n/2+1)的跟随者都表示已经ok后，领导者才能提交日志(回复客户端ok)
3. 领导者要与跟随者保持心跳，当领导者崩溃后，其他跟随者能够通过心跳感知到，并发起新的选举得到新的领导者以维持集群运转。
4. 当有新的节点加入或退出时集群，需要将配置同步给整个集群。
***

开始我们讲过，raft算法之所以好理解，是因为它将一个大问题，拆解成为了若干独立自问题。我这里就大概地把这些问题分为几个子问题来讨论：
1. 领导人选举
2. 日志复制
3. 安全性
4. 日志压缩
5. 成员变更

>在讨论几个子问题前，我们先来看看raft算法涉及到的数据结构/准则/和他的一些特性。

## 数据结构
通用数据结构：

| 参数        | 数据类型 | 释义                                                                         | 持久性 |
| ----------- | -------- | ---------------------------------------------------------------------------- | ------ |
| currentTerm | int      | 节点已知的最新任期号，默认为0，持续递增                                      | 是     |
| votedFor    | int      | 当前节点投票的候选人ID，没有则为空                                           | 是     |
| log         | array    | 日志条目集合，每个条目包含一个用户状态机执行的指令和收到指令时领导者的任期号 | 是     |
| commitIndex | int      | 已知的最大的已经被提交的日志条目索引                                         | 否     |
| lastAppied  | int      | 最后被应用到状态机的日志条目索引，持续递增                                   | 否     |
| nextIndex   | array    | 领导者需要下发给每个节点的下一条日志的索引值，每个节点可能不同。             |        |
| matchIndex  | array    | 对于每一个节点，领导者已经发送给它的日志的最高索引值|        |

日志复制时的发起请求的数据结构：

| 参数        | 数据类型 | 释义                                         |
| ----------- | -------- | -------------------------------------------- |
| term        | int      | 领导人任期号                                 |
| leaderId    | int      | 领导人ID，用于跟随着重定向                   |
| preLogIndex | int      | 新的日志条目的上一条日志索引值               |
| preLogTerm  | int      | preLogIndex条目的任期号                      |
| enteries    | array    | 日志条目，为空时表示心跳，多个是为了提升效率 |
| leaderCommit| int      | 领导人已经提交的日志的索引值    |

日志复制时的响应请求的数据结构：

| 参数        | 数据类型 | 释义                                         |
| ----------- | -------- | -------------------------------------------- |
| term        | int      | 节点已知的最新任期号，用于领导人更新自己任期 |
| success    | bool      | 跟随者包含了匹配上 prevLogIndex 和 prevLogTerm 的日志时为真 |

选举时候选人发起请求的数据结构：

| 参数         | 数据类型 | 释义                         |
| ------------ | -------- | ---------------------------- |
| term         | int      | 候选人的任期号               |
| candidateId  | int      | 请求选票的候选人的 Id        |
| lastLogIndex | int      | 候选人的最后日志条目的索引值 |
| lastLogTerm  | int      | 候选人最后日志条目的任期号   |

选举时响应请求的数据结构：

| 参数         | 数据类型 | 释义                                         |
| ----------- | ------- | -------------------------------------------- |
| term        | int     | 节点已知的最新任期号，以便于候选人去更新自己的任期号 |
| voteGranted | bool    | 候选人赢得了此张选票时为真 |



## 节点准则

### 通用准则
+ 如果commitIndex > lastApplied，那么就 lastApplied 加一，并把log[lastApplied]应用到状态机中。
+ 如果接收到的RPC的请求/响应中，RPC中的任期(term)大于节点的当前任期(currentTerm)，就让currentTerm 等于 term，然后切换状态为跟随者。

### 跟随者准则
+ 响应来自领导者和候选人的请求。
+ 一定时钟周期内(超过选举超时时间)没有收到领导者心跳/附加日志，并且也没有收到候选人投票请求，就把自己变成候选人。
+ 如果接收到客户端的写指令，则重定向到领导者。
+ 如果接收到客户端的读指令：
     + 客户端的最新的term大于节点的term，就重定向到领导者。
     + 客户端的最新的term不大于节点的term，就响应请求。

#### 跟随者准则---选举投票
1.  如果候选人RPC term  < 跟随者currentTerm ，返回false。
2. 如果 votedFor 为空 或者 等于候选人ID，与候选人比较最后一条日志：
     1. 候选人最后一条日志的任期大于自己的，返回true，设置 votedFor 为候选人ID
     2. 候选人最后一条日志的任期小于自己的，返回false。
     3. 候选人最后一条日志的任期等于自己的，比较最后一条日志索引：
          1. 候选人最后一条日志的索引不小于自己，返回true
          2. 候选人最后一条日志的索引小于自己，返回false

#### 跟随者准则---日志复制
1. 如果领导者RPC term  < 跟随者currentTerm ，返回false。
2. 跟随者能否在自己的日志日志中，找到一个与领导者发来的 preLogIndex&preLogTerm 匹配的日志条目--不能就返回false
3. 如果已经跟随者已经存在的日志条目与领导人发过来的日志条目不匹配，那么就删除已经存在的条目和它之后的所有条目。
4. 追加日志中尚未存在的日志条目
5. 如果领导者的已知已经提交的最高日志条目的索引(leaderCommit) 大于 跟随者的已知已经提交的最高日志条目索引(commitIndex)，则把跟随者的已知已经提交的最高日志条目索引(commitIndex) 更新为 领导者的已知已经提交的最高日志条目的索引(leaderCommit) 或是最新的日志条目索引值 中的较小的值。


### 候选人准则
+ 转变成为候选人后马上开始选举过程：
     1. 自增当前任期号 currentTerm
     2. 给自投票
     3. 重置选举超时定时器
     4. 发起投票请求给其他所有节点
+ 接收到大多数(n/2+1)选票就自动成为领导者。
+ 如果接受到来自新的领导者的附加日志请求就转换为跟随者。
+ 选举超时就再次发起选举。

### 领导者准则
1. 一旦成为领导者，就发送空的附加日志(心跳)给其他所有节点表示新的领导者选举成功了(你们的皇帝回来了),之后则按照一定的时钟周期发送心跳信息(空的附加日志)其他所有节点。以抑制其他跟随者成为候选人。
2. 如果接收到来自客户端的写指令:
     1. 附加条目到本地日志中。(commit)
     2. 并行把日志条目发送给其他所有跟随者。
     3. 等待这条日志被附加到大多数跟随者上后回复客户端执行结果。
     4. 将这条日志应用到状态机。(apply)
     5. 对于没有成功的跟随者，不断的发送附加日志直到成功。(cycle)
3. 对于每一个跟随者，领导者附加日志条目时：
     1. 需要 领导者最后条目索引值>= 对应跟随者 nextIndex。
     2. 发送 对应跟随者 nextIndex 开始的所有的日志条目。
     3. 成功则更新对应跟随者的 nextIndex 和 matchIndex 的值。
     4. 如果是因为日志不一致而失败就 减少 nextIndex 重试。
4. 如果存在一个数N，满足条件 N > commitIndex, 并且大多数的 matchIndex[i]>=N 成立，并且log[N].trem == currentTerm 成立， 那么就令 commitIndex 等于这个 N。



## 五条特性
1. 选举安全特性：对于一个给定的任期号，最多只会有一个领导人被选举出来。
2. 领导人只附加原则：领导人绝对不会删除或者覆盖自己的日志，只会增加。
3. 日志匹配原则：如果两个日志在相同的索引位置的日志条目的任期号相同，那么我们就认为这个日志从头到这个索引位置之间全部完全相同。
4. 领导人完全特性：如果某个日志条目在某个任期号中已经被提交，那么这个条目必然出现在更大任期号的所有领导人中。
5. 状态机安全特性：如果一个领导人已经在给定的索引值位置的日志条目应用到状态机中，那么其他任何的服务器在这个索引位置不会提交一个不同的日志。
### 选举安全特性

> 对于一个给定的任期号，最多只会有一个领导人被选举出来.

![[任期.png]](images/任期.png)
#### 证明
1. **因为**在一个任期内，只有得到半数以上的选票(n/2+1)才能当选为领导者。
2. **又因为**在一个任期内，每个节点有且仅有一张选票。
3. **所以**每任期内，有且仅有一个候选人能够获得半数以上选票成为领导者。

### 日志匹配原则

> 如果两个日志在相同的索引位置的日志条目的任期号相同，那么我们就认为这个日志从头到这个索引位置之间全部完全相同

我们可以把这条原则拆为两部分来理解：
1. 如果两个日志在相同的索引位置的日志条目的任期号相同，我们就认为该条目存储的指令相同。
2. 如果两个日志在相同的索引位置的日志条目的任期号相同，我们就认为该条目之前的所有条目都相同。

#### 证明
1. **因为** 选举安全特性，保证了在任意时刻最多有一个领导者。
2. **又因为** 领导人只附加原则，保证了领导人绝对不会删除或者覆盖自己的日志，只会增加。
3. **所以** 只要索引号和任期号相同，我们认为**日志匹配原则1**成立。(即因为数据不会修改，所以索引+任期构成了一条数据的唯一索引)。
4. **因为** 跟随者在接收到附加日志请求时，领导者会把 preLogIndex&preLogTerm 一起发送给跟随者，如果跟随者无法在日志中找到对应日志，就会驳回请求(详情见:**跟随者准则---日志复制**)；直到匹配上日志。
5. **又因为** **日志匹配原则1** 成立。
6. **所以** 匹配上的日志条目之前的日志也是匹配的，因此**日志匹配原则2**成立。

### 领导人完全特性

> 如果某个日志条目在某个任期号中已经被提交，那么这个条目必然出现在更大任期号的所有领导人中。

#### 证明
1. **因为** 候选人只有拥有最新的日志条目才能获取到跟随者的选票。详情见:**跟随者准则---选举投票**
2. **又因为** 当附加日志给跟随者日志前，会先附加到本地日志。详情见:**领导者准则2**
3. **并且** 领导者推进commitIndex 的方式只有当半数以上节点都附加了最新的日志条目才可以推进。详情见:**领导者准则4**
4. **所以** 如果领导人没有宕机，领导人会一直拥有最全的日志，领导人宕机后，当选领导人的候选人也会拥有最全的日志。

#### 缺陷

![[已提交的日志被覆盖.png]](images/已提交的日志被覆盖.png)
假设我们有5个节点：
1. 在a 时刻，S1 是第二任任期领导者， 把日志(任期，索引)(2,2)附加给了S2 后 宕机。
2. 在b 时刻，S5 发起选举，因为日志比和S3/S4一样新，获得选票[S3,S4,S5]3张选票当选第三任领导者。但日志(3,2)未来得及复制给跟随者便宕机。
3. 在c 时刻，S1重新上线，获得S2/S3/S4的选票当选第四任领导人，继续把日志(2,2)复制给S3。此时，日志(2,2)已经附加给了大多数节点。
4. 在d 时刻,  S1再次宕机，S3，凭借(3,2)的日志比较新，赢得选举，任期号为5，然后S5用自己的本地日志为其他节点附加了(3,2)的日志。

所以通过以上例子，我们会发现，仅靠最新日志是无法保证日志的完整性的，它可能会被不同的leader反复覆盖，为了避免这种情况发生。Raft算法为日志提交添加了限制条件：**要求新上任的leader在当前任期至少有一次完整的日志提交(日志条目被复写到大多数集群)，所以，新上任的leader会在接收客户端指令前，先发送一条空指令给其他所有跟随者，来声明自己的领导者地位(这条空指令日志需要被大多数跟随者附加到日志中)，保证选举成立。**
   就像e时刻一样，即使S5重新上线，S1再次宕机，发起选举，因为S1已经将自己任期的日志(4,3)复制给了大多数集群，S5也不能赢得选举。



## 领导者选举
### 状态机
![[节点角色状态机.PNG]](images/节点角色状态机.png)

状态转换说明：
+ 无-->跟随者:集群初始化时，所有节点都是跟随者。
+ 跟随者-->候选人:当跟随者在一定的时钟周期内没有收到来自领导者的信息，就会自动推举自己为候选人。
+ 候选人-->领导者:成为候选人后，候选人会发起竞选，当获取到半数以上的选票时，节点自动成为领导者。
+ 候选人-->跟随者:候选人在投票过程中收到当前任期领导者的心跳信息或是新的任期的领导者的信息时，自动退回到跟随者。
+ 候选人-->候选人:在当前批次的选举中，没有节点竞选成功，同时也没有收到领导者的心跳信息，则再次发起选举。
+ 领导者-->跟随者:当前领导者发现有比自己更高任期的领导者出现时，当前领导者会退回到跟随者。

Raft使用的是心跳机制来触发领导者选举，每个节点在初始化的时候，默认都会把自己的状态设置为跟随者(Follower)，然后随机一个时钟周期(选举定时器)作为心跳超时时间，当在这个时钟周期内，没有收到来自领导者(Leader)的心跳信息，跟随者会切换候选人(Candidate)发起选举过程。详情见**候选人准则**&**跟随者准则---选举投票**

### 性能优化
过快的选举超时时间会导致网络负载，过慢的选举超时时间又会增加故障成本，同样的选举超时时间会导致选票被瓜分。
> 随机选举超时时间，降低选票被瓜分的可能性，而当前经过工程实践验证，随机时间在 150ms～300ms 是最好的。

## 日志复制

> 在领导者没有选举出来前，集群是不接受客户端请求的。

领导者选出后，集群就开始接收来自客户端的请求了，领导者会把请求作为日志条目附加的自己的日志中，然后并行发送给其他所有的跟随者，当这条日志被大多数节点复制以后，领导者会回复客户端响应结果，然后再将日志应用到自己的状态机。

### 同步过程

>  **领导者准则---2,3,4**  &  **跟随者准则---日志复制**。

![[Raft日志同步过程.png]](images/Raft日志同步过程.png)

### 索引推进

>领导者准则4

日志是由有序日志索引(log index)的日志条目组成，每个日志条目都包含有创建时的对应的索引&任期信息&用于状态机执行的指令。如果一条日志被复制到了大多数节点上，我们就认为它是已经被提交了。

![[Raft日志.png]](images/Raft日志.png)

### 冲突解决

> **跟随者准则---日志复制**

![[Leader和Followers上日志不一致.png]](images/Leader和Followers上日志不一致.png)

### 性能优化

#### 流批结合
> 首先可以做的就是 batch，大家知道，在很多情况下面，使用 batch 能明显提升性能，譬如对RocksDB 的写入来说，我们通常不会每次写入一个值，而是会用一个 WriteBatch 缓存一批修改，然后在整个写入。 对于 Raft 来说，Leader 可以一次收集多个 requests，然后一批发送给 Follower。当然，我们也需要有一个最大发送 size 来限制每次最多可以发送多少数据。
> 如果只是用 batch，Leader 还是需要等待 Follower 返回才能继续后面的流程，我们这里还可以使用 Pipeline 来进行加速。大家知道，Leader 会维护一个 NextIndex 的变量来表示下一个给 Follower 发送的 log 位置，通常情况下面，只要 Leader 跟 Follower 建立起了连接，我们都会认为网络是稳定互通的。所以当 Leader 给 Follower 发送了一批 log 之后，它可以直接更新 NextIndex，并且立刻发送后面的 log，不需要等待 Follower 的返回。如果网络出现了错误，或者 Follower 返回一些错误，Leader 就需要重新调整 NextIndex，然后重新发送 log 了。



## 安全性
### 三段论
0. **定义** a 是上个任期领导者最后一条已提交日志， B 为当前任期领导者。
1. **因为** a 是上个任期领导者最后一条已提交日志，则A 必然被同步到了集群中半数以上的节点。
2. 由因为 B 只有获得集群中半数以上的节点的选票才可以当选 领导者(Leader)
3. **所以** B 的选民中必然存在拥有 a 日志的节点。
4. **又因为** **选举限制**，B 只有拥有比其他节点新的日志条目才能获得选票。
5. **所以** B 的日志条目中必然包含日志条目 a。即B 必然包含上任领导人的所有已提交日志。
6. **又因为** 递归理论，每个领导者都包含上任领导者的所有已提交日志。
7. **所以** B 必然拥有集群中最完整的日志。
8. **因为** 集群中只有领导者拥有附加日志的权限。
10. **又因为** **日志完全匹配原则** 。
11. **所以** 领导者附加到每个节点上的日志顺序完全一致。
12. **因为** 节点状态机只能按照日志顺序应用日志。
13. **所以** 状态机在整个集群所有节点上必然 **最终一致**。

### 反证法
1. **假设** 当领导者 A 崩溃以后， 由新节点成为领导者 B ， 但 B 没有包含最后一条已提交日志 a。
2. **因为** 当日志条目a 被附加给集群中半数以上节点时，领导者 A 才会推进commitIndex。
4. **又因为** 选举过程中，只有获得半数以上的节点的选票才可以当选。
5. **所以** 选民中必然有一个包含上任领导者 A 的最后一条已提交日志a。
6. **因为** 选举限制。
7. **所以** 该选民在这种情况不会投票给 领导者B。与假设冲突，所以 B 在没有拥有上任领导者最后一条已提交日志 a 的情况下当选不成立。
8. **所以** 领导者B 要当选，则 B 必然包含上任领导者最后一条已提交日志 a。
9. **所以** 领导者当选的前提是包含所有前任领导者已提交的日志。
10. **又因为** **日志匹配原则**。
11. **所以** 所有节点的已提交日志必然是顺序一致的。
12. **因为** 节点状态机只能按照日志顺序应用日志。
13. **所以** 状态机在整个集群所有节点上必然 **最终一致**。
14. **得证** raft 算法能够保证各个节点上的状态 **最终一致**。


## 日志压缩
实际生产环境中，我们是不可能让日志无限增长的，否则当系统重启或者新节点加入时，需要花费大量的是时间进行回放，从而影响可用性。所以，Raft 采用了对整个系统进行快照(snapshot)的方式来解决。

每个节点独立的对自己的状态机进行快照(snapshot)，然后将之前的日志全部删除。这样在新节点加入或者节点重启时，就节约了大量的回放时间，达到了提升可用性的目的。

在快照(snapshot)时，将会记录：
+ 最后应用的日志的索引和任期，用来保证日志可以匹配。
+ 最后应用的集群配置，用来支持配置更新。

当领导者发送的日志条目因为跟随者落后太多而被丢弃是，领导者将会发送快照(snapshot)给跟随者，来让它更快的跟上集群进度。新的节点加入进来时也是一样。

> 注意: 快照总是由领导者发送给跟随者，发送时按照顺序分块发送。

![[快照.png]](images/快照.png)

### 数据结构
发送请求时：

| 参数              | 数据类型 | 释义                             |
| ----------------- | -------- | -------------------------------- |
| term              | int      | 领导人任期号                     |
| leaderId          | int      | 领导人ID，用于跟随着重定向       |
| lastIncludedIndex | int      | 快照中包含的最后日志条目索引值   |
| lastIncludedTerm  | int      | 快照中包含的最后日志条目任期     |
| offset            | int     | 分块在快照中的字节偏移量         |
| data              | []byte   | 从偏移量开始的快照分块的原始字节 |
| done              | bool     | 是否是最后一个分块  |

### 性能优化
快照何时创建？过于频繁的创建快照会过于浪费性能，过于低频会影响状态机重建时间。
> 限定日志文件大小，达到一定的阈值就触发快照创建。

写入快照花费时间昂贵如何处理？如何保证不影响节点正常工作？
> 使用写时复制技术。


## 成员变更

### 安全性
成员变更指的的集群中的节点发送变化，如新增/删除节点或者节点替换等等。

成员变更也会产生分布式一致性的问题，因为成员变更在集群中达成最终一致性的过程中会影响选举。

它可能在某个时刻因为网络和领导者宕机的原因引起脑裂。如下图：

![[成员变更的某一时刻Cold和Cnew中同时存在两个不相交的多数派.png]](images/成员变更的某一时刻Cold和Cnew中同时存在两个不相交的多数派.png)
0. 在上图中，领导者为S3,原来的成员为S1，S2，新成员为S4，S5。
1. S3 在接收到 S4 & S5 的成员变更请求后，S3 将生成一份 Cnew的配置。
2. 然后 S3 通过附加日志告知了集群中所有节点，但只有S4，S5，响应附加成功。
3. 由于集群中已经有半数以上(5/2+1)的节点附加日志成功，所以S3认为集群配置变更完成。
4. 但在此刻，S3 宕机， 网络形成分区 S1/S2在一个分区，S3/S4/S5在一个分区。
5. S1/S2 由于没有附加日志成功，仍然认为，集群中只有3个节点，所以当其中一个发起选举后，能够获得它们认为大多数选票当选领导者。
6. S3/S4/S5 则因为附加日志成功，但因为S3宕机，选票不足半数，暂时没有领导者当选，但当S3重新上线后，则会有新的领导者当选。
7. 这样，因为一些原因，就导致了集群出现了两个领导者。

所以 raft 算法在成员变更的期间的领导者选举做了新的要求：

> **新集群在配置变更期间，新领导者必须要获得半数以上的老节点的选票才可以当选。**

这样就变成了以下流程：
1. 集群配置发生变更，新节点将配置Cnew发送给领导者。
2. 领导者将自己的Cold配置与Cnew合并为一个Cnew&Cold 配置 【123-45】。
3. 然后发送给其他所有跟随者：
     1. 当同步给半数以上的节点，那么此配置已提交，遵循raft的安全机制。
     2. 未同步给半数以上的节点领导者就宕机，新上任的领导者就会退回老配置，此时客户端重试更新配置即可。
4. 当Cnew&Cold 已经被提交后，领导者会真正的提交Cnew配置【12345】：
     1. 如果提交给了半数以上节点，则 Cnew 提交成功。
     2. 如果未提交成功时领导者宕机，则新选举出来的leader必定包含 Cnew&Cold 配置。那么接着更新配置即可。
5. 如果Cnew更新完成配置后，发现配置中没有自己，自动退出集群就完成了节点删除。

由于两段提交到者集群配置变更变得过于复杂，因此在实现过程中会简化这个步骤，经过严格的数学证明，使用单节点变更机制来规避上述问题(集群中存在两个多数派Cnew/Cold)的发生，即：**每次只新增删除一个节点**。

![[单节点变更.png]](images/单节点变更.png)
通过上图，我们就可以看到：不论旧集群节点数量是奇数还是偶数个，都不会出现同时有两个超过半数以上子集群的存在，也就不可能选出超过一个leader。

raft采用将修改集群配置的命令放在日志条目中来处理，这样做的好处是：
+ 可以继续沿用原来的AppendEntries命令来同步日志数据，只要把修改集群的命令做为一种特殊的命令就可以了。
+   在这个过程中，可以继续处理客户端请求。
### 可用性
#### 添加新节点的集群中
当一个节点添加到集群中，可能会产生一种情况，新的节点的日志可能落后当前集群太多，这种情况下，集群出现的概率的情况就会大大上升：

![[新节点加入.png]](images/新节点加入.png)

上图中，原本是由 S1/S2/S3 组成的3节点集群，这时把节点s4添加进来，而s4上又什么数据都没有。如果此时s3发生故障，在集群中原来有三个节点的情况下，本来可以容忍一个节点的失败的；但是当变成四个节点的集群时，s3和s4同时不可用整个集群就不可用了。

因此Raft算法针对这种新添加进来的节点，是如下处理的：
-   添加进来的新节点首先将不加入到集群中，而是等待数据追上集群的进度。
-   leader同步数据给新节点的流程是，划分为多个轮次，每一轮同步一部分数据，而在同步的时候，leader仍然可以写入新的数据，只要等新的轮次到来继续同步就好。
    以下图来说明同步数据的流程：

![[数据同步流程.png]](images/数据同步流程.png)

如上图中，划分为多个轮次来同步数据。比如，在第一轮同步数据时，leader的最大数据索引为10，那么第一轮就同步10之前的数据。而在同步第一轮数据的同时，leader还能继续接收新的数据，假设当同步第一轮完毕时，最大数据索引变成了20，那么第二轮将继续同步从10到20的数据。以此类推。

这个同步的轮次并不能一直持续下去，一般会有一个限制的轮次数量，比如最多同步10轮。

#### 领导者下线

当需要下线当前集群的领导者的时候，领导者会发出一个配置变更指令，只有当该指令被提交后，旧的领导者才回下线，然后集群会自然的开启新的一轮选举。

#### 跟随者下线

如果某个节点在配置变更后被移除了新的集群，但该节点又没有同步到这个信息，那么根据 raft 算法它就可能会发起新的选举。虽然这个节点最终可能不会赢得选举。但始终对集群产生了干扰，并且这个节点一直不下线，它就会一直对系统造成干扰。

所以为了解决这个问题，Raft引入了一个成为“PreVote”的流程，在这个流程中，如果一个节点要发起一次新的选举，那么首先会广播给集群中的其它所有节点，询问下当前该节点上的日志是否足以赢下选举。只有在这个PreVote阶段赢得超过半数节点肯定的情况下，才真正发起一次新的选举。

然而，PreVote并不能解决所有的问题，因为很有可能该被移除节点上的日志也是最新的。

由于以上的原因，所以不能完全依靠判断日志的方式来决定是否允许一个节点发起新一轮的选举。

Raft采用了另一种机制。如果leader节点一直保持着与其它节点的心跳消息，那么就认为leader节点是存活的，此时不允许发起一轮新的选举。这样follower节点处理RequestVote请求时，就需要加上判断，除了判断请求进行选举的节点日志是否最新以外，如果当前在一段时间内还收到过来自leader节点的心跳消息，那么也不允许发起新的选举。然而这种情况与前面描述的leader迁移的情况相悖，在leader迁移时是强制要求发起新的选举的，因此RequestVote请求的处理还要加上这种情况的判断。

上述机制叫做：惰性领导者(leader stickiness)

#### 惰性领导者
它可以帮我们解决极端情况下网络分区导致的领导者频繁跟换。
![[极端情况下的网络分区.png]](images/极端情况下的网络分区.png)

0. 初始状态，S1是领导者，S2和S3是跟随者。此时，客户端没有发送任何新的请求，所以S1不需要追加任何日志，但它需要定时发送空的附加日志RPC调用作为心跳。注意以下场景仅在集群中所有节点的日志相等时发生。事实上在S1能成功将日志复制到S3的情况下，Raft 可以防止出现领导者频繁切换的场景，因为这将导致S2在它的日志赶上之前，无法赢得选举。
1. S1和S2之间的网络连接断开，每一个节点都认为对方与集群断开连接，然而，他们都能连通S3。
2. 当S2的选举计时器率先超时时，它会增加自己的任期并调用附加 RPC，S3给S2投票，使得S2成为领导者。
3. S1通过S3了解到已经增加的领导者，不再作为Leader。然而它收不到来自S2的任何信息，在选举计时器超时之后，S1增加自己的任期，并发起选举。S3给S1投票使得S1再一次成为领导者。
4. 在网络恢复之前，S1和S2之间的竞争会一直存在。

有很重要的一点需要明白，就是从表面上看，领导者的竞争并不是Raft协议的问题。从表面上看，所有的请求都能得到满足。同一时间只有一个领导者存在，集群也能保证可用。此外，日志匹配属性保持不变，数据的完整性也没有被破坏。

但是在实践中，这种领导者之间的频繁切换并不是我们所期望的。客户端每隔几秒钟就必须重新连接到新的领导者，这种开销是不必要的。某些类型的实现，比如支持长在线，多语句事务，如果领导者的任期比事务提交所需要的时间还短，实际上集群是不可用的。

因此，我们希望一个领导者可以在几天、几周，甚至几个月保持不变，而不是现在的领导者来回切换。

Ratf实际上可以防止一些领导者来回切换的场景。由于领导者需要与集群中的大多数节点日志保持同样新，这就意味着当一个节点或者少数派节点的日志落后之后，他就无法对多数派集群产生影响。

在上面示例描述的不添加新的日志或者至少没有被复制或者提交，这属于极端场景。实际生产环境中可能会因为不稳定的网络导致领导者来回切换，比如网络中断几秒钟，然后恢复，接着又再次中断。与前面的示例相比，这样的网络环境增加了领导者来回切换的可能性，因为断断续续连接断开的服务器有可能追上领导者。在这种情况下，我们期望领导者当选的任期可以尽可能的长，而不是陷入频繁选举和产生新领导者的循环。正是基于这样的考虑，所以有了如下的改进建议:

1. 把选举超时时间加长,使其大于网络闪断时间。触发选举前，这种短暂的网络断开是可以自行恢复的。但是增加选举超时时间加长也会导致故障转移时间加长，所以不太可取。
2. 使领导者变成惰性领导者。

增加“惰性领导者”机制很容易理解：跟随者如果在选举超时时间之前收到过领导者的附加日志请求，则认为领导者工作正常，此时会拒绝请求投票请求。这种情况下：如果集群中的多数派节点与领导者保持连通且工作正常，这时一个有问题的节点或者少数派节点有问题，都不能赢得选举，即：跟随者只会忠诚于当前领导者，(俺是个忠臣，不是内奸)；只有当大多数节点都同意选举时，才会重新发起选举(执政者昏庸无能，群起而殴之)。
